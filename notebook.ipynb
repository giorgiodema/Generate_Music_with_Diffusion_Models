{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Music With Diffusion Models\n",
    "In this notebook we will implement a Diffusion Probabilistic Model to generate music from raw waveform.\\\n",
    "A Diffusion Model learns to invert a Markov process that progressively adds gaussian noise to the signal until the signal is completely destroyed. As shown in the figure.\n",
    "\n",
    "<center><img src=\"./images/diff_model.png\"></img></center>\n",
    "\n",
    "The forward process $q(x_t | x_{t-1})$ is fixed, and it can be parametrized as:\n",
    "\n",
    "$$\n",
    "q(x_t|x_{t-1}) = N(x_t;\\sqrt{1-\\beta_t}x_{t-1},\\beta_t I)\n",
    "$$\n",
    "Where $\\beta_1$,...,$\\beta_t$ are givern by a fixed variance schedule (in our case the betas are set to be linearly increasing constants from $10^{-4}$ to 0.02 as in [1]).\n",
    "\n",
    "If the steps are small enough also the backward process is a gaussian process of the form:\n",
    "\n",
    "$$\n",
    "p(x_{t-1}|x_t) = N(x_{t-1},\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t))\n",
    "$$\n",
    "Where $\\mu_\\theta$ and $\\Sigma_\\theta$ are parametric functions with parameters $\\theta$.\n",
    "\n",
    "Diffusion Models are latent variable models of the form:\n",
    "$$ p_\\theta(x_0) = \\int p_\\theta(x_{0:T}) dx_{1:T}$$\n",
    "where $x_1$,...,$x_T$ are latents with the same dimensionality of the data $x_0 \\sim q(x_0)$.\n",
    "However the distribution $p_\\theta(x_0)$ cannot be optimized directly maximizing the log-lokelihood for the intractability of the integral. Hence we optimize the model maximizing the ELBO:\n",
    "$$\n",
    "E_{x_{1:T}}[\\log{\\frac{p_\\theta(x_{0:T})}{p(x_{1:T}|x_0)}}]\n",
    "$$\n",
    "\n",
    "The maximization of the ELBO can be reduced to the minimization of a loss that compares directly the learned backward process $p_\\theta(x_{t-1}|x_t)$ with the posterior $q(x_{t-1}|x_t,x_0) = N(x_{t-1};\\tilde{\\mu_t}(x_t,x_0),\\tilde{\\beta}_tI)$, that is tractable when conditioned on $x_0$. \n",
    "\n",
    "We adopted the same reparametrization introduced in [1], according to which:\n",
    "$$\n",
    "\\tilde{\\mu_t}(x_t,x_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t}x_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t\n",
    "$$\n",
    "$$\n",
    "\\tilde{\\beta}_t = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t\n",
    "$$\n",
    "$$\n",
    "\\bar{\\alpha_t} = \\prod_{s=1}^t \\alpha_s\n",
    "$$\n",
    "$$\n",
    "\\alpha_t = 1-\\beta_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-24 17:11:59.392813: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from params import params\n",
    "from data.dataset import get_wav,get_unlabelled_dataset\n",
    "import math\n",
    "import subprocess\n",
    "import os\n",
    "from network.model import DiffWaveNet\n",
    "from diffusion.diffusion_process import train,backward_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/media/giorgio/TOSHIBA EXT/ckpt_10\"\n",
    "train_step = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "To train the model we used a small portion of the <a href=\"https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification\">GTZAN</a> dataset (about 40 tracks), because training the model on the whole dataset was too expensive. The GTZAN dataset contains 1000 traks categorized in 10 genres, even though the categorization is not used in this project. \n",
    "### Preprocessing\n",
    "Each sample is a mono audio file (one channel) that is 30 seconds long with a sampling rate of 22.5 kHz. This implies that each sample is a tensor with shape (675000,1). In particular the sequence length was too long to efficiently train the neural network that will be described in the next section. For this reason each audio file was downsampled by a factor 3. In order to prevent aliasing, before the downsampling, the signal was passed through a low pass filter, that was implemented from scratch in order to be fully integrated in the data pipeline. The low pass filter is implemented by the method `data.dataset.low_pass(signal,fc,npoints=128)` that takes as input a signal with shape (seq_len,n_channels) and the cutoff frequency fc and it returns the same signal without all the frequencies component that are above the cutoff. In order to further reduce the number of samples each downsampled audiofile was split into 6 samples.\n",
    "The data pipeline is implemented by the method `data.dataset.get_unlabelled_dataset(bs=None,nsplits=1,downsample=1)`, while the values of the parameters used for training are stored in the file `params.py`.\n",
    "In order to train the model efficiently the dataset was cached into the `dataset/unlabelled_10` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the preprocessed dataset does not exist, then\n",
    "# create it\n",
    "if not os.path.exists(\"dataset/unlabelled_10\"):\n",
    "    ds = get_unlabelled_dataset(params[\"BS\"],nsplits=params[\"NSPLITS\"],downsample=params[\"DOWNSAMPLE\"]).take(10)\n",
    "    tf.data.experimental.save(\n",
    "    ds, \"./dataset/unlabelled_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 4.2.7-0ubuntu0.1 Copyright (c) 2003-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, wav, from 'pipe:':aq=    0KB vq=    0KB sq=    0B f=0/0   \n",
      "  Duration: N/A, bitrate: 117 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 7350 Hz, 1 channels, s16, 117 kb/s\n",
      "   3.56 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B f=0/0   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 4.2.7-0ubuntu0.1 Copyright (c) 2003-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, wav, from 'pipe:':aq=    0KB vq=    0KB sq=    0B f=0/0   \n",
      "  Duration: N/A, bitrate: 117 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 7350 Hz, 1 channels, s16, 117 kb/s\n",
      "   1.37 M-A:  0.000 fd=   0 aq=   12KB vq=    0KB sq=    0B f=0/0   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 4.2.7-0ubuntu0.1 Copyright (c) 2003-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, wav, from 'pipe:':aq=    0KB vq=    0KB sq=    0B f=0/0   \n",
      "  Duration: N/A, bitrate: 117 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 7350 Hz, 1 channels, s16, 117 kb/s\n",
      "   1.77 M-A:  0.000 fd=   0 aq=    7KB vq=    0KB sq=    0B f=0/0   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 4.2.7-0ubuntu0.1 Copyright (c) 2003-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, wav, from 'pipe:':aq=    0KB vq=    0KB sq=    0B f=0/0   \n",
      "  Duration: N/A, bitrate: 117 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 7350 Hz, 1 channels, s16, 117 kb/s\n",
      "   1.18 M-A:  0.000 fd=   0 aq=   16KB vq=    0KB sq=    0B f=0/0   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   1.21 M-A:  0.000 fd=   0 aq=   16KB vq=    0KB sq=    0B f=0/0   \r"
     ]
    }
   ],
   "source": [
    "# set this counter to the number of batches (4 samples each)\n",
    "# you want to listen\n",
    "count = 1\n",
    "\n",
    "ds = tf.data.experimental.load(\"./dataset/unlabelled_10\",tf.TensorSpec(shape=(params[\"BS\"],math.ceil((params[\"NSAMPLES\"]//params[\"NSPLITS\"])/params[\"DOWNSAMPLE\"]),1), dtype=tf.float32))\n",
    "for curr,x in enumerate(ds):\n",
    "    if curr >= count:\n",
    "        break\n",
    "    for i in range(x.shape[0]):\n",
    "        wav = get_wav(x[i],params[\"SR\"]//params[\"DOWNSAMPLE\"])\n",
    "        subprocess.run([\"ffplay\",\"-\"],input=wav.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "In the previous section we stated that the maximization of the ELBO can be reduced to the minimization of a loss that compares directly the learned backward process $p_\\theta(x_{t-1}|x_t)$ with the posterior $q(x_{t-1}|x_t,x_0)$. \n",
    "In particular if we recall that \n",
    "$$\n",
    "p_\\theta(x_{t-1}|x_t) = N(x_{t-1};\\mu_\\theta(x_t,t),\\sigma^2_tI)\n",
    "$$\n",
    "(where we simplified the previous formulation assuming that $\\Sigma_\\theta(x_t,t) = \\sigma^2_tI$ with $\\sigma_t^2 = \\beta_t$) and\n",
    "$$\n",
    "q(x_{t-1}|x_t,x_0) = N(x_{t-1};\\tilde{\\mu_t}(x_t,x_0),\\tilde{\\beta}_tI)\n",
    "$$\n",
    "then the maximization of the ELBO can be reduced to the minimization of:\n",
    "$$\n",
    "E_{t\\sim Uniform(1,T)\\land x_t\\sim q}[\\frac{1}{2\\sigma^2_t}||\\tilde{\\mu}_t(x_t,x_0)-\\mu_\\theta(x_t,t)||^2]\n",
    "$$\n",
    "Moreover, it can be shown that by reparametrizing \n",
    "$$\n",
    "x_t \\sim q(x_t|x_0) = N(x_t;\\sqrt{\\bar{\\alpha_t}}x_0,(1-\\bar{\\alpha_t})I)\n",
    "$$\n",
    "with \n",
    "$$\n",
    "x_t(x_0,\\epsilon) = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon, \\quad\\epsilon\\sim N(0,I)\n",
    "$$\n",
    "than, the loss can be formulated as:\n",
    "$$\n",
    "L = || \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon,t)||^2\n",
    "$$\n",
    "\n",
    "\n",
    "At the end of the day, what we need to implement is a neural network $\\epsilon_\\theta$ that takes as input a sample at any given step t of the forward process $x_t \\sim q(x_t|x_0)$ along with the positional encoding of the step t and predicts the gaussian noise $\\epsilon$ (that has the same dimensionality as $x_t$).\n",
    "\n",
    "The positional encoding is implemented by the function `diffusion.positional_encoder.encode(pos,dmodel,n=10000)` as described in [2] while the neural network is an implementation of the DiffWave network presented in [3] for speech sinthesys.\n",
    "The model architecture is represented in the figure:\n",
    "<center><img src=\"./images/arch.png\" width=600px></img></center>\n",
    "\n",
    "The model was trained with the algorithm in the figure, as in [1]:\n",
    "\n",
    "<center><img src=\"./images/train_alg.png\" width=500px></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "After training, the model can be used to generate new samples from gaussian noise. In particular we sample $x_T \\sim N(0,I)$ and then we repeat the backward process for the fixed number of steps. This process consists in sampling $x_{t-1} \\sim p_\\theta(x_{t-1}|x_t$ starting from x_T and repeating untill x_0 is reached. For simplicity, instead of sampling directly $x_{t-1}$ from $p_\\theta(x_{t-1}|x_t)$ we can use the same reparametrization we introduced before and compute:\n",
    "$$\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha_t}}}\\epsilon_\\theta(x_t,t)) + \\sigma_t z\n",
    "$$\n",
    "with \n",
    "$$\n",
    "z \\sim N(0,1)\n",
    "$$\n",
    "The algorithm is illustrated in the following figure:\n",
    "<center><img src=\"./images/sampl_alg.png\" width=500px></img></center>\n",
    "\n",
    "The sampling algorithm is implemented by the function `diffusion.diffusion_process.backward_process(model,shape,diffusion_steps,return_sequence=False,step_emb_dim:int=128)` that takes as input the neural network, the shape of the sample, and the number of diffusion steps, if return_sequence is set to true the function returns all the intermediate samples $x_t$ with $t \\in [T-1, T-2, ...,0]$. It is important to note that the neural network is composed of convolutional layers, with the exception of the branch that takes as input the positional embedding, which shape does not depend on the sequence length. This implies that the network can generate samples with arbitrary length, independently of the sequence length used for training. For this reason in the `backward_process` function we can specify the shape of the sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "In order to test the correctness of the implementation, the model was first trained on a single batch (four samples) for 1 Million steps. The loss is shown in the following figure:\n",
    "\n",
    "<center><img src=\"./images/test_loss.png\" width=600px></img></center>\n",
    "\n",
    "The weights were saved under the directory `test/model_test/`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-24 17:12:15.108371: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1689600000 exceeds 10% of free system memory.\n",
      "2022-08-24 17:12:20.597816: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1689600000 exceeds 10% of free system memory.\n",
      "2022-08-24 17:12:26.074132: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1689600000 exceeds 10% of free system memory.\n",
      "2022-08-24 17:12:31.544135: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1689600000 exceeds 10% of free system memory.\n",
      "2022-08-24 17:12:37.025502: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1689600000 exceeds 10% of free system memory.\n",
      "ffplay version 4.2.7-0ubuntu0.1 Copyright (c) 2003-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, wav, from 'pipe:':aq=    0KB vq=    0KB sq=    0B f=0/0   \n",
      "  Duration: N/A, bitrate: 117 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 7350 Hz, 1 channels, s16, 117 kb/s\n",
      "  67.47 M-A:  0.000 fd=   0 aq=  106KB vq=    0KB sq=    0B f=0/0   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  67.54 M-A:  0.000 fd=   0 aq=  106KB vq=    0KB sq=    0B f=0/0   \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ffplay', '-'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NSAMPLES = 660000\n",
    "test_net = DiffWaveNet(10,64,3)\n",
    "test_net.load_weights(f\"test/model_test/diff_wave_net\")\n",
    "x_0_gen = backward_process(test_net,(1,NSAMPLES,1),params[\"DIFF_STEPS\"])\n",
    "wav = get_wav(x_0_gen[0],params[\"SR\"]//params[\"DOWNSAMPLE\"])\n",
    "subprocess.run([\"ffplay\",\"-\"],input=wav.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb74053a5f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DiffWaveNet(params[\"DEPTH\"],params[\"CHANNELS\"],params[\"KERNEL_SIZE\"])\n",
    "params[\"MODEL_NAME\"] = net.name\n",
    "if not os.path.exists(ckpt_path):\n",
    "    train(\n",
    "        ds,\n",
    "        params[\"DIFF_STEPS\"],\n",
    "        net,\n",
    "        tf.keras.optimizers.Adam(learning_rate=2*10**-4),\n",
    "        net.name,\n",
    "        ckpt_path=ckpt_path,\n",
    "        print_every=100,\n",
    "        save_every=1000,\n",
    "        resume=False,\n",
    "        resume_ckpt=None\n",
    "    )\n",
    "\n",
    "net.load_weights(os.path.join(ckpt_path,f\"__step_{train_step}__diff_wave_net\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "- [1] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in Neural Information Processing Systems 33 (2020): 6840-6851.\n",
    "- [2] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).\n",
    "- [3] Kong, Zhifeng, et al. \"Diffwave: A versatile diffusion model for audio synthesis.\" arXiv preprint arXiv:2009.09761 (2020)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "342079d9ab4cb8f72e00221ceb32103783b5a02c48115a5173c767386a2e9b2d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tensorflow-cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
